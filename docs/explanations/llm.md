# ðŸ” Deep Dive: LLM Module (Gemini Integration)

## ðŸ§  Role of LLM in RAG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG PIPELINE                                  â”‚
â”‚                                                                  â”‚
â”‚  Query â†’ Retriever â†’ Context â†’ LLM â†’ Answer                    â”‚
â”‚                                  â†‘                               â”‚
â”‚                          THIS MODULE                             â”‚
â”‚                                                                  â”‚
â”‚  The LLM takes retrieved context and generates human answers    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM MODULE                                    â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   LLM_MODELS    â”‚    â”‚         LLMManager               â”‚   â”‚
â”‚  â”‚   (Registry)    â”‚â”€â”€â”€â–ºâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚ ChatGoogleGenerativeAI â”‚    â”‚   â”‚
â”‚                         â”‚   â”‚ _llm (lazy loaded)      â”‚    â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚  LLMProvider    â”‚    â”‚                                   â”‚   â”‚
â”‚  â”‚  (Enum)         â”‚    â”‚   Methods:                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   - get_llm()                     â”‚   â”‚
â”‚                         â”‚   - generate()                    â”‚   â”‚
â”‚                         â”‚   - generate_with_context()       â”‚   â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  Convenience Functions:                                      â”‚â”‚
â”‚  â”‚  - get_llm()  (singleton)                                   â”‚â”‚
â”‚  â”‚  - generate_response()                                      â”‚â”‚
â”‚  â”‚  - list_available_models()                                  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸŽ¯ LLMProvider Enum

```python
class LLMProvider(Enum):
    GEMINI = "gemini"
    GROQ = "groq"  # For future fallback support
```

**Why Enum?**
- Type safety
- IDE autocomplete
- Easy to extend for fallback providers

---

## ðŸ“‹ LLM_MODELS Registry

```python
LLM_MODELS = {
    "gemini-2.5-flash": {
        "provider": LLMProvider.GEMINI,
        "model_name": "gemini-2.5-flash",
        "description": "Fast, efficient for most tasks",
        "context_window": 1_000_000,  # 1 million tokens!
        "free_tier": True,
    },
    "gemini-2.5-pro": {...},
    "gemini-2.0-flash": {...},
}
```

### Model Comparison:

| Model | Speed | Quality | Context | Use Case |
|-------|-------|---------|---------|----------|
| `gemini-2.5-flash` | âš¡âš¡âš¡ | â­â­â­ | 1M | Default, fast responses |
| `gemini-2.5-pro` | âš¡âš¡ | â­â­â­â­ | 1M | Complex reasoning |
| `gemini-2.0-flash` | âš¡âš¡âš¡ | â­â­â­ | 1M | Latest stable |

---

## ðŸ—ï¸ LLMManager Class

### Constructor
```python
def __init__(
    self,
    model_name: str = None,      # Default: gemini-2.5-flash
    temperature: float = None,    # Default: 0.3 (focused)
    max_tokens: int = None,       # Default: 2048
    api_key: Optional[str] = None # From settings or param
):
```

**Configuration from settings.py:**
```python
self.model_name = model_name or settings.default_model
self.temperature = temperature or settings.temperature
self.api_key = api_key or settings.google_api_key
```

### `get_llm()` â€” Lazy Loading
```python
def get_llm(self) -> BaseChatModel:
    if self._llm is None:
        self._llm = self._create_llm()
    return self._llm
```

**Why Lazy?**
```
âŒ Without lazy loading:
import llm  â† API connection made immediately (slow)

âœ… With lazy loading:
import llm  â† Instant
...
llm.generate("Hi")  â† Connection made here, only when needed
```

### `_create_llm()` â€” LangChain Integration
```python
def _create_llm(self) -> ChatGoogleGenerativeAI:
    return ChatGoogleGenerativeAI(
        model=self.model_name,
        temperature=self.temperature,
        max_output_tokens=self.max_tokens,
        google_api_key=self.api_key,
        convert_system_message_to_human=True,  # Gemini quirk
    )
```

**`convert_system_message_to_human=True`:**
Gemini handles system prompts differently. This flag ensures compatibility with LangChain's message format.

---

### `generate()` â€” Basic Generation
```python
def generate(
    self,
    prompt: str,
    system_prompt: Optional[str] = None,
) -> str:
```

**Flow:**
```python
messages = []
if system_prompt:
    messages.append(SystemMessage(content=system_prompt))
messages.append(HumanMessage(content=prompt))

response = llm.invoke(messages)
return response.content
```

**Example:**
```python
manager = LLMManager()
answer = manager.generate(
    "What is RAG?",
    system_prompt="You are a helpful AI teacher."
)
```

---

### `generate_with_context()` â€” RAG-Specific
```python
def generate_with_context(
    self,
    question: str,
    context: str,
    system_prompt: Optional[str] = None,
) -> str:
```

**The Core RAG Pattern:**
```python
prompt = f"""Context:
{context}

Question: {question}

Answer based on the context above:"""
```

**Default System Prompt:**
```
You are a helpful documentation assistant. 
Answer questions based on the provided context. 
If the context doesn't contain the answer, say so honestly.
Be concise but comprehensive.
```

**Usage:**
```python
# After retrieval
context = retriever.retrieve_with_context("refund policy")

# Generate answer using context
answer = llm_manager.generate_with_context(
    question="What is the refund policy?",
    context=context
)
```

---

## ðŸ”„ Singleton Pattern

```python
_default_llm_manager: Optional[LLMManager] = None

def get_llm(model_name=None, temperature=None, force_new=False):
    global _default_llm_manager
    
    if force_new or _default_llm_manager is None:
        _default_llm_manager = LLMManager(...)
    
    return _default_llm_manager.get_llm()
```

**Benefits:**
- Model loaded once per process
- Efficient API connection reuse
- Consistent configuration

---

## ðŸ”— Integration Example

```python
# Complete RAG flow
from src.rag import create_retriever, LLMManager

# 1. Create retriever
retriever = create_retriever()

# 2. Get context
result = retriever.retrieve("What is the refund policy?")
context = result.get_context()

# 3. Generate answer
llm = LLMManager()
answer = llm.generate_with_context(
    question="What is the refund policy?",
    context=context
)

print(answer)
# "Based on the documentation, refunds are processed within 7 days..."
```

---

## ðŸ“‹ Summary

| Component | Pattern | Purpose |
|-----------|---------|---------|
| `LLMProvider` | Enum | Type-safe provider selection |
| `LLM_MODELS` | Registry | Model configurations |
| `LLMManager` | Manager | Configure and use Gemini |
| `get_llm()` | Singleton + Lazy | Efficient model reuse |
| `generate()` | Basic API | Simple text generation |
| `generate_with_context()` | RAG Helper | Context-aware generation |
| `list_available_models()` | Helper | Discover options |
