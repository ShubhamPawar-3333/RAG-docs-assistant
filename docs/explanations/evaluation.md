# ğŸ” Deep Dive: Promptfoo RAG Evaluation

## ğŸ§  Why Promptfoo?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROMPTFOO BENEFITS                            â”‚
â”‚                                                                  â”‚
â”‚  âœ… Declarative YAML â†’ Easy test case management               â”‚
â”‚  âœ… LLM-as-Judge â†’ GPT evaluates answer quality                 â”‚
â”‚  âœ… CI/CD ready â†’ GitHub Actions integration                    â”‚
â”‚  âœ… Multi-provider â†’ Test across different models               â”‚
â”‚  âœ… Free & open â†’ No vendor lock-in                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Evaluation Architecture

```
Golden Test Cases (YAML)
    â†“
Promptfoo Config
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  For each test:                                                  â”‚
â”‚  1. Inject vars into prompt template                            â”‚
â”‚  2. Call LLM provider (Gemini)                                  â”‚
â”‚  3. Run assertions (contains, llm-rubric, relevance)           â”‚
â”‚  4. Record pass/fail, latency, cost                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
JSON Results â†’ Markdown Report
```

---

## ğŸ¯ Key Files

| File | Purpose |
|------|---------|
| `eval/promptfooconfig.yaml` | Main config with prompts & tests |
| `eval/golden_tests.yaml` | Expected Q&A pairs |
| `eval/runner.py` | Python wrapper for automation |
| `eval/results/` | Output directory for results |

---

## ğŸ“‹ Test Categories

### 1. Factual Accuracy
```yaml
- description: "Factual retrieval"
  vars:
    question: "What is RAG?"
    context: "RAG combines retrieval with generation..."
  assert:
    - type: contains
      value: "retrieval"
    - type: llm-rubric
      value: "Answer correctly explains RAG"
```

### 2. Faithfulness (No Hallucination)
```yaml
- description: "Should not hallucinate"
  assert:
    - type: not-contains
      value: "PostgreSQL"
    - type: llm-rubric
      value: "Answer only uses facts from context"
```

### 3. Relevance
```yaml
- description: "Answer should be relevant"
  assert:
    - type: relevance
      threshold: 0.7
```

### 4. Edge Cases
```yaml
- description: "Handle empty context"
  vars:
    question: "What is X?"
    context: ""
  assert:
    - type: llm-rubric
      value: "Admits information is not available"
```

---

## ğŸ”„ Assertion Types

| Type | Purpose |
|------|---------|
| `contains` | Response includes text |
| `not-contains` | Response excludes text |
| `contains-any` | Response includes any of list |
| `llm-rubric` | GPT judges quality |
| `relevance` | Semantic similarity score |
| `cost` | Max cost per query |
| `latency` | Max response time |

---

## ğŸš€ Running Evaluations

```bash
# Install Promptfoo (one-time)
npm install -g promptfoo

# Run evaluation
cd eval
npx promptfoo eval

# View results in browser
npx promptfoo view

# Run with Python wrapper
python eval/runner.py
```

---

## ğŸ“Š Sample Output

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG Evaluation Results                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total Tests: 10                                               â”‚
â”‚ Passed: 9                                                     â”‚
â”‚ Failed: 1                                                     â”‚
â”‚ Pass Rate: 90%                                                â”‚
â”‚ Avg Latency: 1,234ms                                          â”‚
â”‚ Total Cost: $0.0089                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”— CI/CD Integration

```yaml
# .github/workflows/eval.yml
- name: Run RAG Evaluation
  run: |
    npx promptfoo eval -c eval/promptfooconfig.yaml
  env:
    GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
```

---

## ğŸ“‹ Summary

| Component | Purpose |
|-----------|---------|
| `promptfooconfig.yaml` | Test definitions |
| `golden_tests.yaml` | Expected behaviors |
| `runner.py` | Python automation |
| `llm-rubric` | AI-powered evaluation |
| CI/CD | Automated quality checks |
