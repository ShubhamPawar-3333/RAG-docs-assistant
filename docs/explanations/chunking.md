# ğŸ” Deep Dive: Document Chunking Pipeline

## ğŸ§  Why Chunking Matters in RAG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE CHUNKING PROBLEM                          â”‚
â”‚                                                                  â”‚
â”‚  Large Document (50,000 tokens)                                  â”‚
â”‚        â†“                                                         â”‚
â”‚  Embedding Model (max 512 tokens)  â† CAN'T PROCESS!             â”‚
â”‚        â†“                                                         â”‚
â”‚  Solution: Split into smaller chunks                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insight:** Embedding models have token limits. We must split documents into digestible pieces while preserving semantic meaning.

---

## ğŸ“ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STRATEGY PATTERN                              â”‚
â”‚                                                                  â”‚
â”‚  DocumentChunker                                                 â”‚
â”‚       â”‚                                                          â”‚
â”‚       â”œâ”€â”€ RECURSIVE â”€â”€â–º RecursiveCharacterTextSplitter          â”‚
â”‚       â”‚                 (paragraphs â†’ sentences â†’ words)         â”‚
â”‚       â”‚                                                          â”‚
â”‚       â”œâ”€â”€ SEMANTIC â”€â”€â”€â–º RecursiveCharacterTextSplitter          â”‚
â”‚       â”‚                 (with regex sentence detection)          â”‚
â”‚       â”‚                                                          â”‚
â”‚       â””â”€â”€ TOKEN â”€â”€â”€â”€â”€â”€â–º SentenceTransformersTokenTextSplitter   â”‚
â”‚                         (actual token counting)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ The ChunkingStrategy Enum

```python
class ChunkingStrategy(Enum):
    RECURSIVE = "recursive"   # Default, most versatile
    SEMANTIC = "semantic"     # Better sentence preservation
    TOKEN = "token"           # Exact token counting
```

**Why Enum?**
- **Type safety** â€” can't pass invalid strategy
- **IDE autocomplete** â€” easier to use
- **Self-documenting** â€” clear what options exist

---

## ğŸ—ï¸ DocumentChunker Class

### Constructor (`__init__`)

```python
def __init__(
    self,
    chunk_size: int = None,          # Max characters per chunk
    chunk_overlap: int = None,        # Overlap between chunks
    strategy: ChunkingStrategy = ChunkingStrategy.RECURSIVE,
):
```

**The Overlap Concept (Critical for RAG):**

```
WITHOUT OVERLAP:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunk 1 â”‚â”‚  Chunk 2 â”‚â”‚  Chunk 3 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†‘              â†‘
     â””â”€â”€ Information at boundaries is LOST!

WITH OVERLAP (200 chars):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Chunk 1    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚overlap
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Chunk 2    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
             â”‚overlap
        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Chunk 3    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        
â†‘ Sentences at boundaries are preserved in BOTH chunks!
```

**Default values from settings:**
```python
self.chunk_size = chunk_size or settings.chunk_size      # 1000 chars
self.chunk_overlap = chunk_overlap or settings.chunk_overlap  # 200 chars
```

---

### The `_create_splitter` Method (Factory Pattern)

```python
def _create_splitter(self):
    """Create the text splitter based on strategy."""
```

This is a **Factory Method** â€” creates the right object based on configuration.

#### Strategy 1: RECURSIVE (Default, Best for General Use)

```python
return RecursiveCharacterTextSplitter(
    chunk_size=self.chunk_size,
    chunk_overlap=self.chunk_overlap,
    separators=[
        "\n\n",      # 1. Try paragraphs first
        "\n",        # 2. Then line breaks
        ". ",        # 3. Then sentences
        "! ",        # 4. Exclamations
        "? ",        # 5. Questions
        "; ",        # 6. Semicolons
        ", ",        # 7. Commas
        " ",         # 8. Words
        "",          # 9. Characters (last resort)
    ],
)
```

**How Recursive Splitting Works:**

```
Input: "Hello world. This is a test.\n\nNew paragraph here."

Step 1: Try splitting on "\n\n" (paragraphs)
â”œâ”€â”€ "Hello world. This is a test."  â† Chunk 1
â””â”€â”€ "New paragraph here."           â† Chunk 2

If chunks still too big, recursively try next separator...

Step 2: Try splitting on ". " (sentences)
â”œâ”€â”€ "Hello world"                   â† Sub-chunk
â””â”€â”€ "This is a test."               â† Sub-chunk
```

**The hierarchy matters!** We want semantically meaningful breaks.

#### Strategy 2: SEMANTIC (Regex-based Sentence Detection)

```python
separators=[
    "\n\n",                    # Paragraphs
    "\n",                      # Lines
    "(?<=[.!?]) ",             # â† REGEX: Split AFTER sentence-ending punctuation
    " ",                       # Words
    "",                        # Characters
],
is_separator_regex=True,       # Enable regex mode
```

**What `(?<=[.!?]) ` means:**
- `(?<=...)` â€” **Lookbehind assertion** (match position AFTER these chars)
- `[.!?]` â€” Match period, exclamation, or question mark
- ` ` â€” Followed by a space

```
"Hello world. This is great! Right?"
              â†‘              â†‘
              Split HERE     Split HERE
              (after ". ")   (after "! ")
```

This preserves complete sentences better than character-based splitting.

#### Strategy 3: TOKEN (For Model Compatibility)

```python
return SentenceTransformersTokenTextSplitter(
    chunk_overlap=min(self.chunk_overlap, 50),  # Token overlap (not chars)
    tokens_per_chunk=self.chunk_size // 4,      # ~4 chars per token avg
)
```

**Why tokens matter:**
- Models count **tokens**, not characters
- "Hello" = 1 token
- "extraordinary" = 3 tokens (`extra`, `ordin`, `ary`)

This strategy ensures chunks fit exactly in model context windows.

---

### The `chunk_document` Method

```python
def chunk_document(self, document: Document) -> List[Document]:
```

**What it does:**

```python
# 1. Split the document
chunks = self._splitter.split_documents([document])

# 2. Enrich metadata (CRITICAL for RAG!)
for i, chunk in enumerate(chunks):
    chunk.metadata["chunk_index"] = i           # Position in original
    chunk.metadata["total_chunks"] = len(chunks) # Total pieces
    chunk.metadata["chunk_size"] = len(chunk.page_content)
    chunk.metadata["chunking_strategy"] = self.strategy.value
```

**Why metadata enrichment?**

```
User asks: "What's in section 3 of the document?"

Without metadata:
â†’ Can only say "Here's the answer"

With metadata:
â†’ "Here's the answer from policies.pdf (chunk 3 of 10)"
```

---

### The `chunk_documents` Method (Batch Processing)

```python
def chunk_documents(self, documents: List[Document]) -> List[Document]:
    all_chunks: List[Document] = []
    
    for document in documents:
        try:
            chunks = self.chunk_document(document)
            all_chunks.extend(chunks)
        except Exception as e:
            logger.warning(f"Error chunking: {e}")
            continue  # â† Skip failures, process rest
    
    return all_chunks
```

**Design Decisions:**

| Pattern | Why |
|---------|-----|
| `try/except` per doc | One bad doc doesn't kill entire batch |
| `extend` not `append` | Flatten into single list |
| Logging warnings | Track failures without crashing |

---

### The `get_chunk_stats` Method (Debugging Helper)

```python
def get_chunk_stats(self, chunks: List[Document]) -> dict:
    sizes = [len(chunk.page_content) for chunk in chunks]
    
    return {
        "total_chunks": len(chunks),
        "avg_chunk_size": sum(sizes) / len(sizes),
        "min_chunk_size": min(sizes),
        "max_chunk_size": max(sizes),
        "total_characters": sum(sizes),
    }
```

**Use case:**
```python
stats = chunker.get_chunk_stats(chunks)
# {
#   "total_chunks": 47,
#   "avg_chunk_size": 892.3,
#   "min_chunk_size": 156,      â† Maybe too small?
#   "max_chunk_size": 1000,
#   "total_characters": 41938
# }
```

This helps tune chunk_size parameters.

---

## ğŸ”„ Convenience Function

```python
def chunk_documents(
    documents: List[Document],
    chunk_size: int = None,
    chunk_overlap: int = None,
    strategy: str = "recursive"  # â† String, not Enum (simpler API)
) -> List[Document]:
```

**Facade Pattern** â€” simpler interface for common use:

```python
# Without convenience function:
from src.rag.chunking import DocumentChunker, ChunkingStrategy
chunker = DocumentChunker(
    chunk_size=1000,
    strategy=ChunkingStrategy.RECURSIVE
)
chunks = chunker.chunk_documents(docs)

# With convenience function:
from src.rag.chunking import chunk_documents
chunks = chunk_documents(docs, chunk_size=1000, strategy="recursive")
```

---

## ğŸ“Š How Chunking Affects RAG Quality

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CHUNK SIZE TRADEOFFS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  SMALL CHUNKS (200 chars)          LARGE CHUNKS (2000 chars)   â”‚
â”‚  â”œâ”€â”€ More precise                  â”œâ”€â”€ More context             â”‚
â”‚  â”œâ”€â”€ May lose context              â”œâ”€â”€ May include irrelevant   â”‚
â”‚  â”œâ”€â”€ More API calls                â”œâ”€â”€ Fewer API calls          â”‚
â”‚  â””â”€â”€ Risk: incomplete answers      â””â”€â”€ Risk: noisy answers      â”‚
â”‚                                                                 â”‚
â”‚  SWEET SPOT: 500-1000 chars with 10-20% overlap                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”— Integration with Other Modules

```python
# Task 4: Load documents
documents = load_documents("data/docs/")

# Task 5: Chunk documents (THIS MODULE)
chunks = chunk_documents(documents, chunk_size=1000, chunk_overlap=200)

# Task 6: Create embeddings & store in ChromaDB
vector_store = Chroma.from_documents(chunks, embedding_function)

# Task 8: Retrieve relevant chunks
results = vector_store.similarity_search("What is the refund policy?")
```

---

## ğŸ“‹ Summary

| Component | Pattern Used | Purpose |
|-----------|--------------|---------|
| `ChunkingStrategy` | Enum | Type-safe strategy selection |
| `DocumentChunker` | Strategy + Factory | Flexible splitter creation |
| `_create_splitter` | Factory Method | Create right splitter for strategy |
| `chunk_document` | Decorator | Enrich with metadata |
| `chunk_documents` | Iterator | Batch processing with fault tolerance |
| `chunk_documents()` | Facade | Simple one-liner API |
